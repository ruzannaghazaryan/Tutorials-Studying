{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2565a022",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec790c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a338327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab78aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "0  230.1   37.8       69.2   22.1\n",
       "1   44.5   39.3       45.1   10.4\n",
       "2   17.2   45.9       69.3    9.3\n",
       "3  151.5   41.3       58.5   18.5\n",
       "4  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e338ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53d49d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d960f0",
   "metadata": {},
   "source": [
    "## Creating Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b71c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96cb0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_converter = PolynomialFeatures(degree=3, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8987aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = polynomial_converter.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b570c5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_features.shape # 19 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdf7a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef270053",
   "metadata": {},
   "source": [
    "## Splitting into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa90cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd25903",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6f1a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab1aa8",
   "metadata": {},
   "source": [
    "## Standardization of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39c7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4713055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # so now we have this instance of the scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb08f8d",
   "metadata": {},
   "source": [
    "## !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a844ca",
   "metadata": {},
   "source": [
    "And this is where beginners in machine learning often make a crucial mistake. We do not want to assume any information from the test set, otherwise that would result in __data leakage__. So we should __only fit it to the training set__ that we get no information coming in from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6aa3215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8209e3",
   "metadata": {},
   "source": [
    "Once it's fit to the training set, then we can create the scaled versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f6fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f85ff5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(data=X_train[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b07dbea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49300171, -0.33994238,  1.61586707,  0.28407363, -0.02568776,\n",
       "        1.49677566, -0.59023161,  0.41659155,  1.6137853 ,  0.08057172,\n",
       "       -0.05392229,  1.01524393, -0.36986163,  0.52457967,  1.48737034,\n",
       "       -0.66096022, -0.16360242,  0.54694754,  1.37075536])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6f31634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.30100000e+02, 3.78000000e+01, 6.92000000e+01, 5.29460100e+04,\n",
       "       8.69778000e+03, 1.59229200e+04, 1.42884000e+03, 2.61576000e+03,\n",
       "       4.78864000e+03, 1.21828769e+07, 2.00135918e+06, 3.66386389e+06,\n",
       "       3.28776084e+05, 6.01886376e+05, 1.10186606e+06, 5.40101520e+04,\n",
       "       9.88757280e+04, 1.81010592e+05, 3.31373888e+05])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cddbb5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d0b3c",
   "metadata": {},
   "source": [
    "## L2 Regularization - Ridge Regression - Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72c76d",
   "metadata": {},
   "source": [
    "A few important notes to __keep in mind__ when dealing with Scikit-Learn.\n",
    "\n",
    "- The first important note is that internally within the class call for Ridge regression, sklearn refers to __lambda__ as __alpha__.\n",
    "<br>The main reason for that being is the tunable hyperparameters across many different model types within sklearn is just always called alpha. They decided to try to maintain a uniform framework as possible where these __tunable hyperparameters__ are just going to be referred to as __alpha__.\n",
    "- Another important note is, later on, we're actually going to perform cross-validation to choose that best parameter value. And what sklearn uses is something called a __\"scorer object\"__. What are you trying to score by - root mean squared error, mean absolute error, etc. <br>The caveat being is that all scorer objects follow the convention, that __higher return values are better than lower return values__.\n",
    "<br>So what does that actually mean?\n",
    "<br>But what if you're running a regression task, a higher route mean squared error is actually worse.<br> In order to maintain a uniformity across all models and all tasks, what sklearn does is it fixes this by using a negative root mean squared error as a score metric.\n",
    "That means the higher a negative root mean squared error is, the better it actually is.\n",
    "<br>So across any score, the higher it is, the better it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad8c85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e28f5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da388f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Ridge in module sklearn.linear_model._ridge:\n",
      "\n",
      "class Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidge)\n",
      " |  Ridge(alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, solver='auto', positive=False, random_state=None)\n",
      " |  \n",
      " |  Linear least squares with l2 regularization.\n",
      " |  \n",
      " |  Minimizes the objective function::\n",
      " |  \n",
      " |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      " |  \n",
      " |  This model solves a regression model where the loss function is\n",
      " |  the linear least squares function and regularization is given by\n",
      " |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      " |  This estimator has built-in support for multi-variate regression\n",
      " |  (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n",
      " |      Regularization strength; must be a positive float. Regularization\n",
      " |      improves the conditioning of the problem and reduces the variance of\n",
      " |      the estimates. Larger values specify stronger regularization.\n",
      " |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      " |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      " |      :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
      " |      assumed to be specific to the targets. Hence they must correspond in\n",
      " |      number.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to fit the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. ``X`` and ``y`` are expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          ``normalize`` was deprecated in version 1.0 and\n",
      " |          will be removed in 1.2.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  max_iter : int, default=None\n",
      " |      Maximum number of iterations for conjugate gradient solver.\n",
      " |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      " |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      " |      For 'lbfgs' solver, the default value is 15000.\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      Precision of the solution.\n",
      " |  \n",
      " |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
      " |      Solver to use in the computational routines:\n",
      " |  \n",
      " |      - 'auto' chooses the solver automatically based on the type of data.\n",
      " |  \n",
      " |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      " |        coefficients. More stable for singular matrices than 'cholesky'.\n",
      " |  \n",
      " |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      " |        obtain a closed-form solution.\n",
      " |  \n",
      " |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      " |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      " |        more appropriate than 'cholesky' for large-scale data\n",
      " |        (possibility to set `tol` and `max_iter`).\n",
      " |  \n",
      " |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      " |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      " |        procedure.\n",
      " |  \n",
      " |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      " |        its improved, unbiased version named SAGA. Both methods also use an\n",
      " |        iterative procedure, and are often faster than other solvers when\n",
      " |        both n_samples and n_features are large. Note that 'sag' and\n",
      " |        'saga' fast convergence is only guaranteed on features with\n",
      " |        approximately the same scale. You can preprocess the data with a\n",
      " |        scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
      " |        `scipy.optimize.minimize`. It can be used only when `positive`\n",
      " |        is True.\n",
      " |  \n",
      " |      All last six solvers support both dense and sparse data. However, only\n",
      " |      'sag', 'sparse_cg', and 'lbfgs' support sparse input when `fit_intercept`\n",
      " |      is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |  \n",
      " |  positive : bool, default=False\n",
      " |      When set to ``True``, forces the coefficients to be positive.\n",
      " |      Only 'lbfgs' solver is supported in this case.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         `random_state` to support Stochastic Average Gradient.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      " |      Weight vector(s).\n",
      " |  \n",
      " |  intercept_ : float or ndarray of shape (n_targets,)\n",
      " |      Independent term in decision function. Set to 0.0 if\n",
      " |      ``fit_intercept = False``.\n",
      " |  \n",
      " |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      " |      Actual number of iterations for each target. Available only for\n",
      " |      sag and lsqr solvers. Other solvers will return None.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  RidgeClassifier : Ridge classifier.\n",
      " |  RidgeCV : Ridge regression with built-in cross validation.\n",
      " |  :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
      " |      combines ridge regression with the kernel trick.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.linear_model import Ridge\n",
      " |  >>> import numpy as np\n",
      " |  >>> n_samples, n_features = 10, 5\n",
      " |  >>> rng = np.random.RandomState(0)\n",
      " |  >>> y = rng.randn(n_samples)\n",
      " |  >>> X = rng.randn(n_samples, n_features)\n",
      " |  >>> clf = Ridge(alpha=1.0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  Ridge()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Ridge\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      _BaseRidge\n",
      " |      sklearn.linear_model._base.LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, solver='auto', positive=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Ridge regression model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample. If given a float, every sample\n",
      " |          will have the same weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4f19371",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63cbe8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "454b9d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = ridge_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7be45256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "264da7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fb9b208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774404204714177"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38522214",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d172f70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.894638646131968"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe34a40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ab83c",
   "metadata": {},
   "source": [
    "So the main question now is how do we know Alpha is equal to ten was the best choice?\n",
    "\n",
    "Well, we actually don't really know that. What we need to do is use cross validation to test a variety of alpha parameters on cross validated data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2b863f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV   # Ridge Regression with cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21de5b",
   "metadata": {},
   "source": [
    "So what RidgeCV does is it's actually going to perform cross validation for a variety of alpha values. And then what it's going to do is it's going to report back the best alpha value performance based off this cross validation. And recall cross validation in this case is just going to get the average error metric for all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6006faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_model = RidgeCV(alphas=(0.1, 1.0, 10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5e765",
   "metadata": {},
   "source": [
    "So I'm going to do cross validation three separate times with 0.1, 1.0 and then 10.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b1983",
   "metadata": {},
   "source": [
    "- __alphas__ parameter - is the list of alpha values to try, which is the regularization strength parameter.\n",
    "- __cv__ parameter - Determines the cross-validation splitting strategy. It's the integer of how many folds you're going to do in the cross validation. So if you leave it to None, it's going to use the efficient leave one out cross validation. Keep in mind, if you have a huge data set, that could take a lot of time.\n",
    "    <br>Possible inputs for cv are:\n",
    "    - None, to use the efficient Leave-One-Out cross-validation\n",
    "    - integer, to specify the number of folds.\n",
    "- __scoring__ parameter - If None, the negative mean squared error if cv is 'auto' or None\n",
    "    (i.e. when using leave-one-out cross-validation), and r2 score\n",
    "    otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30558ed9",
   "metadata": {},
   "source": [
    "y_test - is our __holdout test set__\n",
    "<br>and cross_validation is going to be done only on X_train - training set.\n",
    "\n",
    "So in reality, this training set, if I'm using a RidgeCV, it technically becomes more than just a training set. \n",
    "It's going to become the training set, plus a small portion of it is going to stay as a validation set in order to check the performance of these three alpha parameters across however many folds you're doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba90bf5",
   "metadata": {},
   "source": [
    "So what's actually happening here is you can think of X_train as that whole data set minus the holdout, and then we're going to do a bunch of folds and get the metrics on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95c71466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([ 0.1,  1. , 10. ]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "076e2e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv_model.alpha_ # it reports back the alpha that performed the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77b1b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "142bbe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCORERS.keys()\n",
    "\n",
    "# And these are all the different types of errors you can try to work with as your scoring metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a0fea",
   "metadata": {},
   "source": [
    "__Keep in mind__ that every single one of these error metrics is transformed so that higher is better. <br>So we can see here instead of saying mean absolute error, we're saying negative mean absolute error, because for normal mean absolute error, the higher the worse. You don't want a very large error. However, if you were to assign a negative mean absolute error, that means higher is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14c9b9",
   "metadata": {},
   "source": [
    "So let's provide this string (neg_mean_absolute_error) as the error metric we're trying to optimize for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b608b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_model = RidgeCV(alphas=(.1, 1.0, 10.0), scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80198078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773c825",
   "metadata": {},
   "source": [
    "I'm checking these three alpha values and I'm scoring them based off negative mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f29e6466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv_model.alpha_ # the best performing alpha out of these three options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ff817",
   "metadata": {},
   "source": [
    "And obviously, if you want, you can keep honing in on alpha and maybe you can say alpha is equal to zero.\n",
    "<br>Maybe no regularization actually makes sense here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43d72483",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = ridge_cv_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf20399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61a8c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26efa0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42737748843313855"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b10461a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6180719926906028"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3fe8a",
   "metadata": {},
   "source": [
    "So now with an alpha value of 0.1 which was found to be the best alpha value according to cross-validation\n",
    "on that training set, if we look at the test set, which is kind of a holdout test set in this case,\n",
    "since I'm not actually tuning based off this test result, I have a mean absolute error of 0.42.\n",
    "If we compare this to the alpha value of ten, that had a mean absolute error of 0.57 on the test set.\n",
    "So I did find a better alpha value with cross validation using RidgeCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04199245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.40769392,  0.5885865 ,  0.40390395, -6.18263924,  4.59607939,\n",
       "       -1.18789654, -1.15200458,  0.57837796, -0.1261586 ,  2.5569777 ,\n",
       "       -1.38900471,  0.86059434,  0.72219553, -0.26129256,  0.17870787,\n",
       "        0.44353612, -0.21362436, -0.04622473, -0.06441449])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875620da",
   "metadata": {},
   "source": [
    "Notice that for L2 regularization, none of these coefficients are zero.\n",
    "Or actually, none of these coefficients are even that close to zero.\n",
    "That's going to be different later on when we take a look at lasso regression, which can actually squeeze\n",
    "coefficients all the way down to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70de552",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2992f",
   "metadata": {},
   "source": [
    "- getting an alpha (or any hyperparameter really) of zero, is a good indication that your predictions aren't improved by regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f065f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
