{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efaace5",
   "metadata": {},
   "source": [
    "- __Ridge regression__ is a regularization technique that works by helping reduce the potential for overfitting to the training data.\n",
    "- And it does this by adding in a penalty term to the error that is based on the squared value of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dee18a0",
   "metadata": {},
   "source": [
    "<img src='rreg1.png' width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e0a78f",
   "metadata": {},
   "source": [
    "Error = RSS + Penalty\n",
    "\n",
    "I'm multiplying the sum of the beta squares by some __tunable lambda parameter__. The Lambda itself will just determine how severe the penalty is, and in theory, it can be any value __from zero to positive infinity__. If it is zero, then it simply reverts back to RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2924b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc974cb",
   "metadata": {},
   "source": [
    "<img src='rreg2.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febea48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a1b06",
   "metadata": {},
   "source": [
    "<img src='rreg3.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29698b37",
   "metadata": {},
   "source": [
    "Training data points are shown in this pink hue and my test points are shown as blue hue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceeced5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be740f5",
   "metadata": {},
   "source": [
    "With Ridge regression we're trying to make sure we don't overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed56f5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e448f",
   "metadata": {},
   "source": [
    "now we go ahead and just grab the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5227cad",
   "metadata": {},
   "source": [
    "<img src='rreg4.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df9211",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e38f2b",
   "metadata": {},
   "source": [
    "<img src='rreg5.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9a256",
   "metadata": {},
   "source": [
    "So then you get your fitted line only minimising RSS (without penalty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e01e76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a0f3f",
   "metadata": {},
   "source": [
    "<img src='rreg6.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f6ad5",
   "metadata": {},
   "source": [
    "This means we have high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46d27d",
   "metadata": {},
   "source": [
    "This line, it looks a little too steep as the data starts to kind of have lower Y values, even as X increases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253e9b6",
   "metadata": {},
   "source": [
    "So what does that actually mean when you over fit the training data?\n",
    "It means you have high variance instead of a higher bias.\n",
    "So you're essentially fitting to a lot more noise in the training data than actually generalizing to the entire data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b92330",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd285ed5",
   "metadata": {},
   "source": [
    "Well, the idea behind Ridge regression is can we actually introduce a little more __bias__ in order to significantly\n",
    "__reduce__ variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9978fc",
   "metadata": {},
   "source": [
    "<img src='rreg7.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d06997",
   "metadata": {},
   "source": [
    "So we can see here we have this blue line that has a little more bias and the reason we can say has more bias because you notice it's not fitting as well to the actual red points in the data set. So it's more biased and it's not picking up as much noise or variance.\n",
    "So it's going to get not as good of a fit to the training data, but the idea would be that when it comes to new unseen data, overall, it gives you a better fit kind of in the long term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd8f46",
   "metadata": {},
   "source": [
    "So how does this actually work mathematically?<br>\n",
    "Let's imagine we're trying to reduce the ridge regression error term and the ridge regression error\n",
    "term is that same RSS plus the shrinkage penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c4d92",
   "metadata": {},
   "source": [
    "<img src='rreg8.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c71c06",
   "metadata": {},
   "source": [
    "- If we assume Lambda is equal to 1, then really what we're trying to minimize is the beta coefficients and the beta coefficient squared.\n",
    "- So what this actually does in the case of a single feature x is if we're trying to minimize this beta\n",
    "coefficient and the squared of this beta coefficient, then it's going to start punishing it if you\n",
    "have too large of a slope.\n",
    "- lambda parameter would just be a tuning factor of how much do you want to punish by beta squared. But regardless of lambda, as long as it's not equal to zero, we're still going to punish those larger\n",
    "slopes. So again, for a single feature, this would actually lower your slope value, at the cost of some additional bias, which would show up as error in your training set. But we're going to generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211f6dc",
   "metadata": {},
   "source": [
    "<img src='rreg9.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96fe6f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321afb7f",
   "metadata": {},
   "source": [
    "<img src='rreg10.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ff212",
   "metadata": {},
   "source": [
    "Consider that you figured out your overfitting to the training set.\n",
    "That means if you're overfitting to the training set and you have a single feature X, then an increase in x results in a greater y response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322828ef",
   "metadata": {},
   "source": [
    "<img src='rreg11.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef3407",
   "metadata": {},
   "source": [
    "So note here that distance change in x denoted by that green arrow, would end up significantly increasing with a greater Y response.\n",
    "\n",
    "If we add in the shrinkage penalty, that, in the case of one feature, is punishing a large slope value because it's essentially the slope squared that is trying to minimize, then we get this less steep line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602de0a1",
   "metadata": {},
   "source": [
    "<img src='rreg12.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f72618",
   "metadata": {},
   "source": [
    "If you're punishing the squared slope of the line, realistically you're going to end up with\n",
    "a smaller slope.\n",
    "And that smaller slope means you're not quickly jumping as far along the Y.\n",
    "\n",
    "So this is really the cusp of what ridge regression is trying to do.\n",
    "\n",
    "It's trying to make sure that you're not overly responsive to your training data.\n",
    "<br>That way, when new unknown data comes in, you have a little more biased and you're going to generalize\n",
    "better to new unseen data.\n",
    "\n",
    "So you're going to punish something that's really steep, even if it fits well to the training data.\n",
    "So hopefully that leads to less overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363c316",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758bb67",
   "metadata": {},
   "source": [
    "What about that lambda term? How much should we punish these larger coefficients?\n",
    "\n",
    "I just said that lambda can be anything from zero up to positive infinity. That's quite a range to choose from.\n",
    "\n",
    "How do we actually choose the best lambda?\n",
    "<br>Well, we can simply use cross validation.\n",
    "What we'll do is we'll explore multiple lambda options and then just choose the best one based off the\n",
    "performance metrics that we already know about, such as mean absolute error and root mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b25a47",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f03c9",
   "metadata": {},
   "source": [
    "- Since L2 is reducing Beta coeffs, this means its reducing slopes (keep in mind, we can't really visualize anything more than 3 separate Betas). Now since slopes/betas are being reduced which means the Beta value being multiplied by a feature is being reduced, which means that the features will not have as strong of an effect than without regularization. As you noted, this particular visualization is constructed in a way to visually show the effect, but in general, you just have to make the connection that:\n",
    "\n",
    "reduced slopes --> reduced Betas --> reduced effect from features on Y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c699384",
   "metadata": {},
   "source": [
    "- if a beta coefficient shrinking due to the shrinkage penalty actually caused a less accurate predictive model, then we would naturally find that the optimal lambda value should be 0, and in that case we would discover that regularization is not warranted in that particular model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75719ba",
   "metadata": {},
   "source": [
    "- Overfitting is related to higher beta coefficients, since they make your model more sensitive. So with L2 regularization you are severely punishing precisely those high beta coefficients that make your model way too sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65160725",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770514d8",
   "metadata": {},
   "source": [
    "1. So we use ridge regression when we are overfitting?\n",
    "\n",
    "Yes, or you can try both and then just compare performance metrics\n",
    "\n",
    "2. Ridge regression is a method to reduce overfitting adding penalties?\n",
    "\n",
    "Yes, by not letting Beta coefficients treat any single feature too strongly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df4797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
