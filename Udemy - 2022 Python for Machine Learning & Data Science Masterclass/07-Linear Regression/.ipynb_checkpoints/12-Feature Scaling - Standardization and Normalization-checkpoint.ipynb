{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced1036e",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a6b20",
   "metadata": {},
   "source": [
    "- Some machine learning models that rely on distance metrics (e.g. KNN) require scaling to perform well.\n",
    "- Feature scaling improves the convergence of the steepest descent algorithms (algorithms like gradient descent in trying to minimize the lost function), which do not possess the property of scale invariance.\n",
    "- There are some machine learning algorithms where scaling is not going to have any effect (e.g. CART based models_ Classification and Regression Trees).\n",
    "- If you just train the model to feature data that was scaled, that means for new incoming unseen data, you will also have to scale it before feeding it to the model.\n",
    "- If you scale all the features to the same range, that does make it easier to compare one coefficient to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722bc18",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2075b6",
   "metadata": {},
   "source": [
    "- Feature Scaling Benefits:\n",
    "    - Can lead to great increases in performance.\n",
    "    - Absolutely necessary for some models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa47c3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46421edc",
   "metadata": {},
   "source": [
    "<img src='fs1.png' width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c72db4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526b198",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379d1be",
   "metadata": {},
   "source": [
    "It means you're essentially standardizing all your data to follow a standard normal distribution so you could have negative values there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e6120",
   "metadata": {},
   "source": [
    "<img src='fs2.png' width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a1034",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004400b5",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba34ba",
   "metadata": {},
   "source": [
    "<img src='fs3.png' width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba589cd9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec228dc",
   "metadata": {},
   "source": [
    "# .fit() and .transform() methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81dea7c",
   "metadata": {},
   "source": [
    "<img src='fs4.png' width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a600594",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a976c",
   "metadata": {},
   "source": [
    "<img src='fs5.png' width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534e72e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc67bab",
   "metadata": {},
   "source": [
    "<img src='fs6.png' width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b13a52",
   "metadata": {},
   "source": [
    "leakage - утечка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f607d7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215e1a2",
   "metadata": {},
   "source": [
    "# Feature Scaling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41384b20",
   "metadata": {},
   "source": [
    "<img src='fs7.png' width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4369a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685377f",
   "metadata": {},
   "source": [
    "# Do we need to scale the label?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81975e27",
   "metadata": {},
   "source": [
    "- In general it is NOT necessary nor advised. \n",
    "- Normalizing the output distribution is altering the definition of the target.\n",
    "- Predicting a distribution that doesn't mirror your real-world target.\n",
    "- Can actually negatively impact stochastic gradient descent.\n",
    "<br>https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244632e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97dcdb6",
   "metadata": {},
   "source": [
    "__Standardization:__\n",
    "\n",
    "Standardizing the features around the center and 0 with a standard deviation of 1 is important when we compare measurements that have different units. Variables that are measured at different scales do not contribute equally to the analysis and might end up creating a bais.\n",
    "\n",
    "For example, A variable that ranges between 0 and 1000 will outweigh a variable that ranges between 0 and 1. Using these variables without standardization will give the variable with the larger range weight of 1000 in the analysis. Transforming the data to comparable scales can prevent this problem. Typical data standardization procedures equalize the range and/or data variability.\n",
    "\n",
    "__Normalization:__\n",
    "\n",
    "Similarly, the goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization. It is required only when features have different ranges.\n",
    "\n",
    "For example, consider a data set containing two features, age, and income(x2). Where age ranges from 0–100, while income ranges from 0–100,000 and higher. Income is about 1,000 times larger than age. So, these two features are in very different ranges. When we do further analysis, like multivariate linear regression, for example, the attributed income will intrinsically influence the result more due to its larger value. But this doesn’t necessarily mean it is more important as a predictor. So we normalize the data to bring all the variables to the same range.\n",
    "\n",
    "__When Should You Use Normalization And Standardization:__\n",
    "\n",
    "Normalization is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve). Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks.\n",
    "\n",
    "Standardization assumes that your data has a Gaussian (bell curve) distribution. This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian. Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688c4eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452fa0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
